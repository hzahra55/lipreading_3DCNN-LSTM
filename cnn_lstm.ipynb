{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8FJWDXUUTFk",
        "outputId": "49a3c70a-7f25-4450-96e3-5416622fc22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-6d5622ec9ce2>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is GPU available? False\n",
            "GPU Device: []\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"Is GPU available?\", tf.test.is_gpu_available())\n",
        "print(\"GPU Device:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqFgTxd2UVQb",
        "outputId": "78f11054-d3f9-4649-fd2c-5743d9336102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset extracted to: /content/saved_datasets\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset path in Google Drive\n",
        "drive_dataset_path = \"/content/drive/MyDrive/saved_datasets.zip\"\n",
        "\n",
        "# Create a directory for extraction\n",
        "output_dir = \"/content/saved_datasets\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile(drive_dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(f\"Dataset extracted to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP212RXCUVMo",
        "outputId": "c5365bc8-d93f-4f7e-9384-4e623f495a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_datasets\n"
          ]
        }
      ],
      "source": [
        "!ls /content/saved_datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6h1nYlkUVKF",
        "outputId": "4624f01d-a712-4191-90eb-2cd6111a9e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset paths:\n",
            "Train: /content/saved_datasets/saved_datasets/train_dataset\n",
            "Validation: /content/saved_datasets/saved_datasets/val_dataset\n",
            "Test: /content/saved_datasets/saved_datasets/test_dataset\n"
          ]
        }
      ],
      "source": [
        "# Correct Dataset Paths\n",
        "base_dir = \"/content/saved_datasets/saved_datasets\"\n",
        "train_path = os.path.join(base_dir, \"train_dataset\")\n",
        "val_path = os.path.join(base_dir, \"val_dataset\")\n",
        "test_path = os.path.join(base_dir, \"test_dataset\")\n",
        "\n",
        "print(\"Dataset paths:\")\n",
        "print(f\"Train: {train_path}\")\n",
        "print(f\"Validation: {val_path}\")\n",
        "print(f\"Test: {test_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpvX29oLnzIK",
        "outputId": "f4f2cb7c-c3ae-4e2b-8f27-cc0ed617f335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8264606385342128425  dataset_spec.pb  snapshot.metadata\n",
            "12001490812919011829  dataset_spec.pb  snapshot.metadata\n",
            "12262735734570148572  dataset_spec.pb  snapshot.metadata\n"
          ]
        }
      ],
      "source": [
        "!ls /content/saved_datasets/saved_datasets/train_dataset\n",
        "!ls /content/saved_datasets/saved_datasets/val_dataset\n",
        "!ls /content/saved_datasets/saved_datasets/test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAhaJh8dZ6nB",
        "outputId": "88f50b0b-d86f-44e7-861a-2f481a9c6420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-93240ced7f7b>:4: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.load(...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully!\n",
            "Train Sequence Shape: (8, 32, 224, 224, 3)\n",
            "Train Label: [170 430 433 226 466 512 194 499]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = tf.data.experimental.load(train_path)\n",
        "val_dataset = tf.data.experimental.load(val_path)\n",
        "test_dataset = tf.data.experimental.load(test_path)\n",
        "\n",
        "print(\"Datasets loaded successfully!\")\n",
        "\n",
        "# Verify one batch\n",
        "for sequence, label in train_dataset.take(1):\n",
        "    print(\"Train Sequence Shape:\", sequence.shape)\n",
        "    print(\"Train Label:\", label.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd_9fHGMZ6ZT"
      },
      "outputs": [],
      "source": [
        "# model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr4HGYHRr1Bu",
        "outputId": "6ec852bb-0e8f-466e-b6d0-e56d01a46915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsXKAY_tr2s1"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/trained_words.txt /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VHexYhSr2el",
        "outputId": "9bed8a60-5a9d-4a08-f409-0a3d8de1ed02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 547\n"
          ]
        }
      ],
      "source": [
        "# Load trained words from the file\n",
        "def load_trained_words(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        return [line.strip() for line in f.readlines()]\n",
        "\n",
        "trained_words = load_trained_words(\"/content/trained_words.txt\")\n",
        "\n",
        "# Initialize the tokenizer\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "        self.index_to_word = {idx: word for word, idx in self.word_to_index.items()}\n",
        "\n",
        "    def __call__(self, word):\n",
        "        return self.word_to_index.get(word, 0)  # 0 for unknown words (PAD)\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.word_to_index)\n",
        "\n",
        "tokenizer = SimpleTokenizer(trained_words)\n",
        "\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YqMj52zt-Lt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Add a blank token to the vocabulary\n",
        "blank_token = len(tokenizer.word_to_index)  # Assign the last index for the blank token\n",
        "tokenizer.word_to_index[\"<BLANK>\"] = blank_token\n",
        "num_classes = len(tokenizer.word_to_index)  # Update the total number of classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "440WEYpft954"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # def prepare_ctc_inputs(dataset):\n",
        "# #     \"\"\"\n",
        "# #     Prepares inputs for CTC loss by computing input and label lengths.\n",
        "\n",
        "# #     Args:\n",
        "# #         dataset: A TensorFlow dataset with (sequence, label) pairs.\n",
        "\n",
        "# #     Returns:\n",
        "# #         processed_dataset: A TensorFlow dataset with (inputs, labels, input_length, label_length).\n",
        "# #     \"\"\"\n",
        "# #     def map_fn(sequence, label):\n",
        "# #         # Compute input length: fixed length TARGET_SEQ_LEN for all inputs\n",
        "# #         input_length = tf.fill([tf.shape(sequence)[0]], TARGET_SEQ_LEN)\n",
        "\n",
        "# #         # Ensure label length and label shapes are compatible\n",
        "# #         label_length = tf.fill([tf.shape(label)[0]], 1) if len(label.shape) == 1 else tf.shape(label)\n",
        "\n",
        "# #         # Expand label dimensions if necessary to match required shape [batch_size, max_label_length]\n",
        "# #         if len(label.shape) == 1:\n",
        "# #             label = tf.expand_dims(label, axis=-1)\n",
        "# #         print(\"Sequence shape:\", tf.shape(sequence))\n",
        "# #         print(\"Label shape:\", tf.shape(label))\n",
        "# #         print(\"Input Length:\", input_length)\n",
        "# #         print(\"Label Length:\", label_length)\n",
        "\n",
        "# #         return ((sequence, input_length), (label, label_length))\n",
        "\n",
        "# #     return dataset.map(map_fn)\n",
        "# def prepare_ctc_inputs(dataset):\n",
        "#     \"\"\"\n",
        "#     Prepares inputs for CTC loss by computing input and label lengths.\n",
        "\n",
        "#     Args:\n",
        "#         dataset: A TensorFlow dataset with (sequence, label) pairs.\n",
        "\n",
        "#     Returns:\n",
        "#         processed_dataset: A TensorFlow dataset with (inputs, labels, input_length, label_length).\n",
        "#     \"\"\"\n",
        "#     def map_fn(sequence, label):\n",
        "#         # Compute input length\n",
        "#         input_length = tf.fill([tf.shape(sequence)[0]], TARGET_SEQ_LEN)  # Assuming fixed TARGET_SEQ_LEN\n",
        "\n",
        "#         # Compute label length\n",
        "#         label_length = tf.expand_dims(tf.shape(label)[-1], axis=-1)  # Ensure it's compatible with stacking\n",
        "\n",
        "#         # Expand label dimensions if necessary\n",
        "#         if len(label.shape) == 1:\n",
        "#             label = tf.expand_dims(label, axis=-1)\n",
        "\n",
        "#         # Combine label and label_length\n",
        "#         y_true = tf.concat([label, label_length], axis=-1)  # Concatenate along the last dimension\n",
        "\n",
        "#         return ((sequence, input_length), y_true)\n",
        "\n",
        "#     return dataset.map(map_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "suQQDdp8yFZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_ctc_inputs(dataset):\n",
        "#     \"\"\"\n",
        "#     Prepares inputs for CTC loss by computing input and label lengths.\n",
        "\n",
        "#     Args:\n",
        "#         dataset: A TensorFlow dataset with (sequence, label) pairs.\n",
        "\n",
        "#     Returns:\n",
        "#         processed_dataset: A TensorFlow dataset with (inputs, labels, input_length, label_length).\n",
        "#     \"\"\"\n",
        "#     def map_fn(sequence, label):\n",
        "#         # Compute input length: fixed length TARGET_SEQ_LEN for all inputs\n",
        "#         input_length = tf.fill([1], TARGET_SEQ_LEN)\n",
        "\n",
        "#         # Compute label length\n",
        "#         label_length = tf.shape(label)[0]  # Get the length of the label sequence\n",
        "\n",
        "#         # Expand label_length to match the rank of label\n",
        "#         label_length = tf.expand_dims(label_length, axis=0)  # Shape becomes [1]\n",
        "\n",
        "#         # Ensure label has the right shape\n",
        "#         # if len(label.shape) == 1:\n",
        "#         #     label = tf.expand_dims(label, axis=-1)  # Shape becomes [seq_length, 1]\n",
        "\n",
        "#         # No need to concatenate label and label_length here\n",
        "#         # y_true = tf.concat([label, tf.tile(label_length, [tf.shape(label)[0]])], axis=-1)\n",
        "\n",
        "#         return ((sequence, input_length), (label, label_length))\n",
        "\n",
        "#     return dataset.map(map_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "HvXRln_Ey21P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tebYzHBmuK_M"
      },
      "outputs": [],
      "source": [
        "# @tf.function\n",
        "# def ctc_loss(y_true, y_pred):\n",
        "#     \"\"\"\n",
        "#     Compute the CTC loss for the model.\n",
        "\n",
        "#     Args:\n",
        "#         y_true: Labels and their lengths. This should be a tuple of (labels, label_lengths)\n",
        "#         y_pred: Logits output by the model.\n",
        "\n",
        "#     Returns:\n",
        "#         loss: Scalar tensor representing the mean CTC loss.\n",
        "#     \"\"\"\n",
        "#     # Unpack y_true\n",
        "#     labels, label_length = y_true\n",
        "\n",
        "#     # Debug input shapes\n",
        "#     tf.print(\"Labels shape (input):\", tf.shape(labels))\n",
        "#     tf.print(\"Label lengths shape (input):\", tf.shape(label_length))\n",
        "#     tf.print(\"y_pred shape (input):\", tf.shape(y_pred))\n",
        "\n",
        "#     # Cast labels and label_lengths to the correct dtype\n",
        "#     labels = tf.cast(labels, tf.int32)\n",
        "#     label_length = tf.cast(label_length, tf.int32)\n",
        "\n",
        "#     # Debug extracted shapes\n",
        "#     tf.print(\"Labels shape:\", tf.shape(labels))\n",
        "#     tf.print(\"Label lengths shape:\", tf.shape(label_length))\n",
        "\n",
        "#     # Transpose y_pred for CTC loss\n",
        "#     y_pred = tf.transpose(y_pred, perm=[1, 0, 2])  # Transpose to (time_steps, batch_size, num_classes)\n",
        "\n",
        "#     # Debug y_pred shape after transpose\n",
        "#     tf.print(\"y_pred shape (transposed):\", tf.shape(y_pred))\n",
        "\n",
        "#     # Compute input lengths\n",
        "#     batch_size = tf.shape(y_pred)[1]\n",
        "#     input_length = tf.fill([batch_size], tf.shape(y_pred)[0])  # Time steps (32)\n",
        "\n",
        "#     # Debug input lengths\n",
        "#     tf.print(\"Input lengths shape:\", tf.shape(input_length))\n",
        "\n",
        "#     # Compute CTC loss\n",
        "#     loss = tf.nn.ctc_loss(\n",
        "#         labels=labels,\n",
        "#         logits=y_pred,\n",
        "#         label_length=label_length,\n",
        "#         logit_length=input_length,\n",
        "#         blank_index=-1,  # Assuming you are using TensorFlow 2.x\n",
        "#         logits_time_major=True\n",
        "#     )\n",
        "\n",
        "#     return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZtNII0IjpQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MPMMZhkWPn3d",
        "outputId": "a7f1bea3-cfc1-4eb6-ce2e-b4c74a71eaaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fZagQtILpKvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo pip3 install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zP2zx8DQV7S",
        "outputId": "7790c349-f2f1-49a0-9191-3d46ae421d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ewC9byXZ8mZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d446b4f0-3ae4-457d-b6f4-350ac6e0c6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# from tensorflow.keras import layers, models, Input, regularizers\n",
        "\n",
        "# # Define input shape: (time_steps, height, width, channels)\n",
        "# input_shape = (32, 224, 224, 3)  # 32 frames, 224x224 resolution, RGB channels\n",
        "# num_classes = tokenizer.vocab_size()  # Total number of unique classes\n",
        "\n",
        "# # Input layer\n",
        "# inputs = Input(shape=input_shape)\n",
        "\n",
        "# # 3D CNN Layers\n",
        "# x = layers.Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu',\n",
        "#                   padding='same', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
        "# x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "# x = layers.Dropout(0.3)(x)\n",
        "\n",
        "# x = layers.Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu',\n",
        "#                   padding='same', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "# x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "# x = layers.Dropout(0.3)(x)\n",
        "\n",
        "# x = layers.Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu',\n",
        "#                   padding='same', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "# x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "# x = layers.Dropout(0.4)(x)\n",
        "\n",
        "# # Reshape for LSTM Layer\n",
        "# x = layers.TimeDistributed(layers.Flatten())(x)  # Flatten each frame\n",
        "# x = layers.Reshape((32, -1))(x)  # Reshape to (time_steps, features)\n",
        "\n",
        "# # LSTM Layer\n",
        "# lstm_out = layers.Bidirectional(layers.LSTM(256, return_sequences=False, dropout=0.5))(x)\n",
        "\n",
        "# # Dense Layer for Final Output\n",
        "# outputs = layers.Dense(num_classes, activation='softmax')(lstm_out)\n",
        "\n",
        "# # Create the model\n",
        "# model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers, models, Input, regularizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Define input shape: (time_steps, height, width, channels)\n",
        "input_shape = (32, 224, 224, 3)  # 32 frames, 224x224 resolution, RGB channels\n",
        "num_classes = tokenizer.vocab_size()  # Total number of unique classes\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "# Pretrained ResNet50 for Feature Extraction (applied to each frame)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze pretrained layers\n",
        "\n",
        "# Apply ResNet50 to each frame using TimeDistributed\n",
        "x = layers.TimeDistributed(base_model)(inputs)\n",
        "x = layers.TimeDistributed(layers.Flatten())(x)  # Flatten the ResNet50 output\n",
        "\n",
        "# Add Temporal Attention Mechanism\n",
        "attention = layers.Attention()([x, x])\n",
        "\n",
        "# LSTM Layers\n",
        "x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.5))(attention)\n",
        "x = layers.Bidirectional(layers.LSTM(256, return_sequences=False, dropout=0.5))(x)\n",
        "\n",
        "# Fully Connected Layers\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)  # Additional Dropout for regularization\n",
        "\n",
        "# Output Layer\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR43ZOmLZ8Vx",
        "outputId": "a78f924b-9d03-4b43-d585-09970b8e8a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 224, 224, 3)]    0         []                            \n",
            "                                                                                                  \n",
            " time_distributed (TimeDist  (None, 32, 7, 7, 2048)       2358771   ['input_1[0][0]']             \n",
            " ributed)                                                 2                                       \n",
            "                                                                                                  \n",
            " time_distributed_1 (TimeDi  (None, 32, 100352)           0         ['time_distributed[0][0]']    \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 32, 100352)           0         ['time_distributed_1[0][0]',  \n",
            "                                                                     'time_distributed_1[0][0]']  \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  (None, 32, 512)              2060472   ['attention[0][0]']           \n",
            " al)                                                      32                                      \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  (None, 512)                  1574912   ['bidirectional[0][0]']       \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 512)                  262656    ['bidirectional_1[0][0]']     \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 512)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 548)                  281124    ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 231753636 (884.07 MB)\n",
            "Trainable params: 208165924 (794.09 MB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Summary of the Model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCBEu8tCAgWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke8I33WEo_Z4"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # Integer labels\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bV6kdXQvfFG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Prepare dataset for CTC training\n",
        "# train_dataset_ctc = prepare_ctc_inputs(train_dataset)\n",
        "# val_dataset_ctc = prepare_ctc_inputs(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if label is tensor"
      ],
      "metadata": {
        "id": "FiNLEMhT2IXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_dataset_ctc.take(1):\n",
        "#     _, (label,_) = batch\n",
        "#     print( isinstance(label, tf.Tensor) and label.dtype != tf.variant)\n"
      ],
      "metadata": {
        "id": "ryusJNCn2LYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_dataset_ctc.take(1):\n",
        "#     (x, x_len), (y, y_len) = batch\n",
        "\n",
        "#     # Debug input shapes\n",
        "#     print(\"Input shape (x):\", tf.shape(x))\n",
        "#     print(\"Input length (x_len):\", x_len)\n",
        "\n",
        "#     # Debug label shapes\n",
        "#     print(\"Label shape (y):\", tf.shape(y))\n",
        "#     print(\"Label length (y_len):\", y_len)\n"
      ],
      "metadata": {
        "id": "FsLcJAKw3x0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-RV-qS8wEqV",
        "outputId": "30b4999e-3a99-4bde-b6d6-cee344305814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence contains NaN: False\n",
            "Label Data Type: <dtype: 'int32'>\n",
            "Sequence contains NaN: False\n",
            "Label Data Type: <dtype: 'int32'>\n",
            "Sequence contains NaN: False\n",
            "Label Data Type: <dtype: 'int32'>\n",
            "Sequence contains NaN: False\n",
            "Label Data Type: <dtype: 'int32'>\n",
            "Sequence contains NaN: False\n",
            "Label Data Type: <dtype: 'int32'>\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataset.take(5):\n",
        "    sequence, label = batch\n",
        "\n",
        "    # Check for NaN only in sequences (assumed float32)\n",
        "    print(\"Sequence contains NaN:\", tf.math.reduce_any(tf.math.is_nan(sequence)).numpy())\n",
        "    print(\"Label Data Type:\", label.dtype)  # Confirm label type (should be int32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohwxMJaHyZzd",
        "outputId": "62372b36-9799-45c5-d2d5-068d7335fe91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating Train Dataset:\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 170, Max = 512\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 12, Max = 526\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 6, Max = 503\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 6, Max = 483\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.0039061899296939373\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 11, Max = 540\n",
            "✅ Labels are in the valid range.\n",
            "\n",
            "Validating Validation Dataset:\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 175, Max = 540\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 110, Max = 540\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 11, Max = 516\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.0030296039767563343\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 72, Max = 485\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003813917748630047\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 21, Max = 523\n",
            "✅ Labels are in the valid range.\n",
            "\n",
            "Validating Test Dataset:\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 29, Max = 468\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 159, Max = 544\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 208, Max = 544\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 6, Max = 544\n",
            "✅ Labels are in the valid range.\n",
            "Sequence Range: Min = 0.0, Max = 0.003921568859368563\n",
            "✅ Sequence values are in the range [0, 1].\n",
            "Label Range: Min = 236, Max = 516\n",
            "✅ Labels are in the valid range.\n"
          ]
        }
      ],
      "source": [
        "# Check ranges of sequences and labels in the dataset\n",
        "import tensorflow as tf\n",
        "\n",
        "def validate_dataset(dataset, num_classes):\n",
        "    \"\"\"\n",
        "    Validates that sequences are normalized (0-1) and labels are in the valid range.\n",
        "\n",
        "    Args:\n",
        "        dataset: TensorFlow dataset to validate.\n",
        "        num_classes: The total number of classes (num_classes - 1 is the maximum valid label).\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    for batch in dataset.take(5):  # Check first 5 batches\n",
        "        sequences, labels = batch\n",
        "\n",
        "        # Check sequence range\n",
        "        min_val = tf.reduce_min(sequences).numpy()\n",
        "        max_val = tf.reduce_max(sequences).numpy()\n",
        "        print(f\"Sequence Range: Min = {min_val}, Max = {max_val}\")\n",
        "\n",
        "        if min_val < 0.0 or max_val > 1.0:\n",
        "            print(\"⚠️ Sequence values are out of range! Expected [0, 1].\")\n",
        "        else:\n",
        "            print(\"✅ Sequence values are in the range [0, 1].\")\n",
        "\n",
        "        # Check label range\n",
        "        min_label = tf.reduce_min(labels).numpy()\n",
        "        max_label = tf.reduce_max(labels).numpy()\n",
        "        print(f\"Label Range: Min = {min_label}, Max = {max_label}\")\n",
        "\n",
        "        if min_label < 0 or max_label >= num_classes:\n",
        "            print(f\"⚠️ Labels are out of range! Expected [0, {num_classes-1}].\")\n",
        "        else:\n",
        "            print(\"✅ Labels are in the valid range.\")\n",
        "\n",
        "# Number of classes (vocabulary size)\n",
        "num_classes = len(tokenizer.word_to_index)\n",
        "\n",
        "# Run validation on datasets\n",
        "print(\"Validating Train Dataset:\")\n",
        "validate_dataset(train_dataset, num_classes)\n",
        "\n",
        "print(\"\\nValidating Validation Dataset:\")\n",
        "validate_dataset(val_dataset, num_classes)\n",
        "\n",
        "print(\"\\nValidating Test Dataset:\")\n",
        "validate_dataset(test_dataset, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_dataset_ctc.take(1):\n",
        "#     (x, x_len), (y, y_len) = batch\n",
        "#     print(\"Input shape (x):\", tf.shape(x))         # [batch_size, max_time, feature_dim]\n",
        "#     print(\"Input length (x_len):\", x_len)         # [batch_size]\n",
        "#     print(\"Label shape (y):\", tf.shape(y))        # [batch_size, max_label_length]\n",
        "#     print(\"Label length (y_len):\", y_len)         # [batch_size]\n",
        "\n"
      ],
      "metadata": {
        "id": "3PO39MRJjrUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# # Reshape the data before fitting the model\n",
        "# def reshape_data(sequence, label):\n",
        "#   \"\"\"Reshapes the sequence to match the expected input shape.\"\"\"\n",
        "#   sequence = tf.expand_dims(sequence, axis=0)  # Add a batch dimension\n",
        "#   sequence = tf.tile(sequence, [8, 1, 1, 1, 1])  # Repeat along time dimension\n",
        "#   return sequence, label\n",
        "\n",
        "# train_dataset = train_dataset.map(reshape_data)\n",
        "# val_dataset = val_dataset.map(reshape_data)\n"
      ],
      "metadata": {
        "id": "iD4vDH81AzTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE6q0dWEpGvK",
        "outputId": "644c28a4-f6fb-4ff4-e2cd-702181b043f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  7/167 [>.............................] - ETA: 3:44:21 - loss: 6.3422 - accuracy: 0.0000e+00"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataset.take(1):\n",
        "    x_batch, y_batch = batch\n",
        "    print(f\"Input batch shape: {x_batch.shape}\")  # Expected: (batch_size, 32, 224, 224, 3)\n",
        "    print(f\"Label batch shape: {y_batch.shape}\")  # Expected: (batch_size,)\n"
      ],
      "metadata": {
        "id": "aZk5CfHaEhXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging the dataset shape\n",
        "for batch in train_dataset.take(1):\n",
        "    x_batch, y_batch = batch\n",
        "    print(f\"Input batch shape: {x_batch.shape}\")  # Expected: (batch_size, 32, 224, 224, 3)\n",
        "    print(f\"Label batch shape: {y_batch.shape}\")  # Expected: (batch_size,)\n"
      ],
      "metadata": {
        "id": "MEeDaucyCNDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.int32)))\n"
      ],
      "metadata": {
        "id": "XY8ruFKuCTC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLaWmtMWpMHh"
      },
      "outputs": [],
      "source": [
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check shape of tensor"
      ],
      "metadata": {
        "id": "mBiKIpZjSzTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM-LDkJdpM6j"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Iterate through the first batch\n",
        "# for batch in train_dataset_ctc.take(2):\n",
        "#     ((sequence, input_length), (label, label_length)) = batch  # Unpack the batch\n",
        "\n",
        "#     # print(\"Sequence:\")\n",
        "#     # print(sequence.numpy()) # Convert to NumPy array for printing\n",
        "#     print(\"Sequence Shape:\", sequence.shape)\n",
        "#     print(\"Input Length:\", input_length.numpy())\n",
        "\n",
        "#     print(\"\\nLabel:\")\n",
        "#     print(label.numpy())\n",
        "#     print(\"Label Shape:\", label.shape)\n",
        "#     print(\"Label Length:\", label_length.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoVsq7tIOpyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}